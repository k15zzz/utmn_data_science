{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Аттестационное задание 6 по теме “Определение фальшивых новостей с использованием NLP” состоит из следующих задач: (разработано РУКОН)"
      ],
      "metadata": {
        "id": "7lSK3fdKIpml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Определение требований и целей системы"
      ],
      "metadata": {
        "id": "Iu61PVODJH-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Система, которую разрабатываем, предназначена для анализа текстов новостей и определения их достоверности. Это важная задача, учитывая растущую проблему фальшивых новостей в современном мире. Фальшивые новости могут искажать общественное мнение, влиять на политические процессы и вызывать социальные напряжения. Поэтому создание эффективной системы для борьбы с фальшивыми новостями является важной задачей.\n",
        "\n",
        "Цель системы - обеспечить точное и быстрое определение достоверности новостей. Система должна быть способна обрабатывать большие объемы данных и работать в реальном времени. Она должна быть достаточно гибкой, чтобы справляться с различными типами новостей и источниками информации.\n",
        "\n",
        "Система должна предназначена для использования в новостных агентствах, отделах СМИ государственных и муниципальных орнагизаций, социальных сетях и других платформах, где распространяются новости. Она должна быть простой в использовании и интеграции с существующими системами.\n",
        "\n",
        "Основные требования к системе включают:\n",
        "•\tТочность определения достоверности новостей.\n",
        "•\tСпособность обрабатывать большие объемы данных.\n",
        "•\tБыстродействие и способность работать в реальном времени.\n",
        "•\tГибкость и способность адаптироваться к различным типам новостей и источникам информации.\n",
        "•\tПростоту использования и интеграции с существующими системами.\n",
        "\n",
        "Основные риски и проблемы, связанные с использованием системы:\n",
        "\n",
        "•\tОшибки в определении достоверности новостей. Отсутствкует алгоритм, который был бы 100% точным, поэтому всегда есть риск ошибки.\n",
        "\n",
        "•\tПроблемы с обработкой больших объемов данных. Это может потребовать больших вычислительных ресурсов и привести к замедлению работы системы.\n",
        "\n",
        "•\tВозможные нарушения конфиденциальности. Система должна обеспечивать защиту личной информации и соблюдать законы о защите данных.\n",
        "\n",
        "•\tВозможность манипуляции системой. Например, злоумышленники могут попытаться \"обучить\" систему неправильно определять достоверность новостей.\n",
        "\n",
        "В качестве результата этой задачи мы представим документ с подробным описанием требований и целей системы, а также возможных рисков и проблем."
      ],
      "metadata": {
        "id": "9Spz1w00JWge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка и обработка данных для обучения модели NLP\n"
      ],
      "metadata": {
        "id": "I_bAKxeQRj1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Подготовка данных - это критически важный этап в любом проекте машинного обучения. В контексте NLP и определения фальшивых новостей, это включает в себя сбор подходящего набора данных, очистку данных от шума и аномалий, применение техник предобработки данных, таких как токенизация и лемматизация, и разделение данных на обучающую, валидационную и тестовую выборки.\n",
        "\n",
        "Сбор данных может включать в себя поиск существующих наборов данных о фальшивых новостях или сбор данных из различных источников, таких как новостные сайты и социальные сети. Важно убедиться, что данные представляют собой баланс между фальшивыми и реальными новостями и что они отражают разнообразие тем и стилей написания.\n",
        "\n",
        "Очистка данных может включать в себя удаление дубликатов, исправление ошибок в тексте и удаление нерелевантных частей текста, таких как заголовки и метаданные. Это помогает убедиться, что модель обучается только на релевантных данных.\n",
        "\n",
        "Предобработка данных включает в себя токенизацию, то есть разбиение текста на отдельные слова или токены, и лемматизацию, то есть приведение слов к их базовой форме. Это помогает уменьшить размерность данных и улучшить качество модели.\n",
        "\n",
        "Разделение данных на обучающую, валидационную и тестовую выборки помогает убедиться, что модель обучается на одних данных, настраивается на других и тестируется на третьих, независимых данных. Это помогает предотвратить переобучение и дает более надежную оценку качества модели."
      ],
      "metadata": {
        "id": "BkSVXErHRRrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Предобработка**"
      ],
      "metadata": {
        "id": "ebBio59kbiLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Загрузка данных\n",
        "data = pd.read_csv('fakenews.csv')\n",
        "\n",
        "# Удаление строк с неопределенными значениями\n",
        "data = data.dropna()\n",
        "\n",
        "# Очистка данных от шума и аномалий\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data['text'] = data['text'].apply(lambda x: \"\".join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "# Предобработка данных\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply(lambda x: word_tokenize(x))\n",
        "data['text'] = data['text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "data['text'] = data['text'].apply(lambda x: ''.join(x))  # Объединение токенов обратно в строки\n",
        "\n",
        "texts = data['text'].values\n",
        "labels = data['label'].values\n",
        "\n",
        "# Преобразование меток в числовой формат\n",
        "label_map = {'FAKE': 0, 'REAL': 1}\n",
        "labels = np.array([label_map[label] for label in labels])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FWwCFFpirqq",
        "outputId": "60ad4c62-552c-4ce3-c2c3-e2087592b3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Разработка алгоритма для анализа текста новостей и определения их достоверности"
      ],
      "metadata": {
        "id": "7KqjDXwtJlig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Одной из ключевых задач в области обработки естественного языка (NLP) является анализ текста и определение его достоверности. В контексте новостей это особенно важно, поскольку распространение фальшивых новостей может иметь серьезные последствия, включая искажение общественного мнения и влияние на политические процессы.\n",
        "\n",
        "Для решения этой задачи мы можем использовать комбинацию различных методов NLP и машинного обучения. Во-первых, нам нужно преобразовать текст новостей в числовой формат, который можно использовать для обучения модели. Это можно сделать с помощью методов векторизации текста, таких как TF-IDF, Word2Vec или Doc2Vec. Эти методы преобразуют текст в векторы, которые отражают важность слов в тексте и их контекст.\n",
        "\n",
        "В нашем случае мы выбрали Word2Vec, поскольку он учитывает контекст слов в тексте и может дать более точные результаты, чем TF-IDF, который учитывает только важность слов. Word2Vec создает векторы слов, которые расположены близко друг к другу в векторном пространстве, если они часто встречаются вместе в тексте.\n",
        "\n",
        "После векторизации текста мы можем использовать алгоритмы машинного обучения для классификации новостей на достоверные и фальшивые. Возможные варианты включают наивный байесовский классификатор, SVM, Random Forest и Gradient Boosting. Мы выбрали Random Forest, поскольку он обычно дает хорошие результаты в задачах классификации и может обрабатывать большие объемы данных.\n",
        "\n",
        "Random Forest работает путем создания множества деревьев решений и объединения их прогнозов. Это делает его более устойчивым к переобучению, чем отдельные деревья решений.\n",
        "\n",
        "Для оценки качества нашей модели мы используем кросс-валидацию. Это метод, при котором данные разделяются на несколько подмножеств, и модель обучается и тестируется несколько раз, каждый раз на разных подмножествах данных. Это дает более надежную оценку качества модели, чем простое разделение на обучающую и тестовую выборки.\n",
        "\n",
        "Для реализации нашего алгоритма мы используем Python и несколько библиотек, включая pandas для обработки данных, gensim для векторизации текста, sklearn для машинного обучения и nltk для предварительной обработки текста.\n",
        "\n",
        "В результате мы получаем алгоритм, который может анализировать текст новостей и определять их достоверность с высокой точностью. Этот алгоритм может быть использован в новостных агентствах, социальных сетях и других платформах для борьбы с распространением фальшивых новостей."
      ],
      "metadata": {
        "id": "kMwX_K0cJwy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Алгоритм:**"
      ],
      "metadata": {
        "id": "7P4v-UcnJ_Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# # Не получается векторизация текста с помощью Word2Vec и правильная ли она\n",
        "# w2v_model = Word2Vec(texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# # Получение векторов для каждого слова в предложении и их усреднение\n",
        "# def get_sentence_vector(words_list, model):\n",
        "#     vector_sum = sum(model.wv[word] for word in words_list if word in model.wv.key_to_index)\n",
        "#     return vector_sum / len(words_list)\n",
        "\n",
        "# X = pd.Series(texts).apply(lambda x: get_sentence_vector(x, w2v_model))\n",
        "\n",
        "# Преобразование текстов в числовые векторы с помощью TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=3000)\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Обучение модели Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "01hbg1Cii6Ct",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "acbc718f-c584-4783-d6d8-ab6c9244dbde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Оценка качества и эффективности системы\n"
      ],
      "metadata": {
        "id": "CD_IXwFgRiqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "После подготовки данных и обучения модели следующим шагом является оценка ее качества и эффективности. Это включает в себя тестирование модели на тестовой выборке и оценку ее по различным метрикам, таким как точность, полнота и F-мера.\n",
        "\n",
        "Тестирование модели на тестовой выборке дает нам возможность увидеть, как модель работает на новых, ранее не виденных данных. Это важно, поскольку модель, которая хорошо работает на обучающих данных, может плохо работать на новых данных, если она переобучена.\n",
        "\n",
        "Оценка модели по различным метрикам дает нам более полное представление о ее качестве. Например, точность показывает, как часто модель правильно определяет фальшивые новости, а полнота показывает, сколько из всех фальшивых новостей модель смогла обнаружить."
      ],
      "metadata": {
        "id": "DkK4F2fuRabX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)"
      ],
      "metadata": {
        "id": "Ke2NOVtya_Of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec6782f-d3b5-43d8-b2e9-615bbdc1a359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7730496453900709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print('Recall:', recall)"
      ],
      "metadata": {
        "id": "2d6K3kkBbAdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2aa4fa-f83f-4210-8d8b-fb4f414f59a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.6753246753246753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print('F1-score:', f1)"
      ],
      "metadata": {
        "id": "brdIgVY1bBh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d3320e-5d4b-47a7-92be-6062740361b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-score: 0.7647058823529411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель Random Forest, обученная на векторах, полученных с помощью TfidfVectorizer, показала хорошие результаты в задаче классификации текстов на тональность.\n",
        "\n",
        "Для улучшения модели можно попробовать следующие подходы:\n",
        "\n",
        "- Использовать другой алгоритм векторизации текста, например, Doc2Vec или BERT.\n",
        "\n",
        "- Использовать другой классификатор, например, SVM или XGBoost.\n",
        "\n",
        "- Попробовать увеличить количество деревьев в модели Random Forest.\n",
        "\n",
        "- Попробовать увеличить количество признаков в модели TfidfVectorizer.\n",
        "\n",
        "- Попробовать увеличить количество слов в каждом предложении.\n",
        "\n",
        "- Попробовать увеличить данные модели.\n",
        "\n",
        "- Попробовать использовать другие методы для улучшения модели, например, методы регуляризации или методы выбора признаков.\n",
        "\n",
        "- Попробовать использовать другие данные для обучения модели, например, данные из других источников или данные с другими метками тональности.\n"
      ],
      "metadata": {
        "id": "VV4xEGiQEeBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Интеграция системы в существующие продукты или сервисы"
      ],
      "metadata": {
        "id": "QJbubwYORgad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "После того, как модель была обучена и протестирована, следующим шагом является интеграция системы в существующие продукты или сервисы. Это может включать в себя интеграцию с новостными сайтами для автоматической проверки новостей на достоверность, интеграцию с социальными сетями для обнаружения и блокировки фальшивых новостей, или интеграцию с поисковыми системами для улучшения качества результатов поиска.\n",
        "\n",
        "Интеграция системы может принести множество преимуществ, включая улучшение качества информации, уменьшение распространения фальшивых новостей и улучшение пользовательского опыта. Однако она также может представлять сложности, такие как необходимость адаптации системы к специфическим требованиям продукта или сервиса, обеспечение масштабируемости и производительности системы, и управление вопросами конфиденциальности и безопасности данных."
      ],
      "metadata": {
        "id": "h7_lPm-ORe-l"
      }
    }
  ]
}